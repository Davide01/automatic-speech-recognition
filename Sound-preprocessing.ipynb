{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading https://files.pythonhosted.org/packages/09/b4/5b411f19de48f8fc1a0ff615555aa9124952e4156e94d4803377e50cfa4c/librosa-0.6.2.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 400kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting audioread>=2.0.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/41/8cd160c6b2046b997d571a744a7f398f39e954a62dd747b2aae1ad7f07d4/audioread-2.1.6.tar.gz\n",
      "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/site-packages (from librosa)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/site-packages (from librosa)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/site-packages (from librosa)\n",
      "Collecting joblib>=0.12 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/1b/995167f6c66848d4eb7eabc386aebe07a1571b397629b2eac3b7bebdc343/joblib-0.13.0-py2.py3-none-any.whl (276kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 809kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/site-packages (from librosa)\n",
      "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/site-packages (from librosa)\n",
      "Collecting resampy>=0.2.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/b6/66a06d85474190b50aee1a6c09cdc95bb405ac47338b27e9b21409da1760/resampy-0.2.1.tar.gz (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 705kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.38.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/42/45/8d5fc45e5f760ac65906ba48dec98e99e7920c96783ac7248c5e31c9464e/numba-0.40.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 258kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting llvmlite>=0.25.0dev0 (from numba>=0.38.0->librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/34/fb/f9c2e9e0ef2b54c52f0b727cf6af75b68c3d7ddb6d88c8d557b1b16bc1ab/llvmlite-0.25.0-cp36-cp36m-manylinux1_x86_64.whl (16.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.1MB 79kB/s eta 0:00:01   24% |████████                        | 4.0MB 738kB/s eta 0:00:17    64% |████████████████████▌           | 10.3MB 1.5MB/s eta 0:00:04    85% |███████████████████████████▎    | 13.7MB 1.1MB/s eta 0:00:03    93% |██████████████████████████████  | 15.0MB 686kB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, audioread, resampy\n",
      "  Running setup.py bdist_wheel for librosa ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/18/b8/10/f0f8f6ac60668a5cd75596cf14c25bb6b3ea1ecd815f058b7e\n",
      "  Running setup.py bdist_wheel for audioread ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/53/02/90/7b5c4081b7470c550ab605f600bad237dde12a6b8999b11f50\n",
      "  Running setup.py bdist_wheel for resampy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ff/4f/ed/2e6c676c23efe5394bb40ade50662e90eb46e29b48324c5f9b\n",
      "Successfully built librosa audioread resampy\n",
      "Installing collected packages: audioread, joblib, llvmlite, numba, resampy, librosa\n",
      "Successfully installed audioread-2.1.6 joblib-0.13.0 librosa-0.6.2 llvmlite-0.25.0 numba-0.40.1 resampy-0.2.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for file manipulation\n",
    "def load_samples(file_path):\n",
    "    ys, srs = [[]],[[]]\n",
    "    i = 0\n",
    "    #loads .wav files\n",
    "    for filename in os.listdir(file_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            y, sr = librosa.load(path+filename, sr=16000)\n",
    "            ys[i].append(y)\n",
    "            srs[i].append(sr)\n",
    "            i = i + 1\n",
    "            ys.append([])\n",
    "            srs.append([])  \n",
    "    ys = ys[0: len(ys) - 1]\n",
    "    srs = srs[0: len(srs) - 1]\n",
    "    return (ys, srs)\n",
    "def load_labels(file_path):\n",
    "    i = 0\n",
    "    labels = [[]]\n",
    "    \n",
    "    for filename in os.listdir(file_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file = open(file_path+filename, \"r\") \n",
    "            labels[i].append(file.read())\n",
    "            labels.append([])\n",
    "            i = i + 1\n",
    "            \n",
    "    labels=labels[0: len(labels) - 1]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for feature extraction\n",
    "def find_max(ys):\n",
    "    maximum =0\n",
    "    for y1 in ys:\n",
    "        for y2 in y1:\n",
    "            dim = y2.shape[0]\n",
    "            if dim > maximum:\n",
    "                maximum = dim\n",
    "    return maximum\n",
    "\n",
    "def pad_signal(ys):\n",
    "    max_length = find_max(ys)\n",
    "    \n",
    "    ys_new = ys\n",
    "    for i, y1 in enumerate(ys_new):\n",
    "        for j, y2 in enumerate(y1):\n",
    "            if len(y2) < max_length:\n",
    "                z = numpy.zeros((max_length - len(y2)))\n",
    "                pad_signal = numpy.append(y2, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "                ys_new[i][j] = pad_signal\n",
    "    return ys_new\n",
    "def pre_emphasize(ys, pre_emphasis):\n",
    "    for i, y in enumerate(ys):\n",
    "        signal=y[0]\n",
    "        y[0] = numpy.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    return ys\n",
    "def fourier_transform(ys, N_FFT=512, window='hamming', hop_size=256):\n",
    "\n",
    "    Ds = [[]]\n",
    "\n",
    "    for i, y in enumerate(ys):\n",
    "        Ds[i].append(librosa.core.stft(y=y[0], n_fft=N_FFT, window=window, hop_length=hop_size))\n",
    "        Ds.append([])\n",
    "    return Ds\n",
    "\n",
    "def mfccs(ys, N_FFT=512, sr=16000, n_mfcc=40, hop_size=256):\n",
    "    mels = [[]]\n",
    "    \n",
    "    for i, y in enumerate(ys):\n",
    "        mels[i].append(librosa.feature.mfcc(y[0], sr=sr, n_mfcc=n_mfcc, n_fft=N_FFT, hop_length=hop_size))\n",
    "        mels.append([])\n",
    "    return mels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "* we open the samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"an4_dataset/train/\"#path to the dataset\n",
    "\n",
    "ys, srs = load_samples(path)\n",
    "labels = load_labels(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "* loaded data is preprocessed\n",
    "* we perform stft using librosa\n",
    "* then we add melspectrogram\n",
    "* and MFCCs, all are prepared, but we can use each of them, so we get different kinds of features\n",
    "\n",
    "**Note**: For stft we have a window size, typically 512 or 256 and hop size. On each iteration we start at \n",
    "$$\n",
    "n_1 = N_f x H\n",
    "$$\n",
    "and we finish at\n",
    "$$\n",
    "n_2 = n_1 + M - 1\n",
    "$$\n",
    "\n",
    "https://dsp.stackexchange.com/questions/38491/time-position-in-stft-output\n",
    "\n",
    "H is a hop size (length) and M is a window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_new = pad_signal(ys)\n",
    "\n",
    "ys_emphasized=pre_emphasize(ys_new, 0.97)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FFT = 400 #window size\n",
    "window = 'hamming'\n",
    "hop_size = 160\n",
    "N_MFCC = 40\n",
    "\n",
    "Ds=fourier_transform(ys=ys_emphasized, N_FFT=N_FFT, window=window, hop_size=hop_size)\n",
    "Ms=mfccs(ys=ys_emphasized, n_mfcc=N_MFCC, N_FFT=N_FFT, hop_size=hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_1): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (l_1): Linear(in_features=640, out_features=100, bias=True)\n",
      "  (l_out): Linear(in_features=100, out_features=27, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperameters of the model\n",
    "num_classes = 27\n",
    "channels = 1\n",
    "height = 1\n",
    "width = 40\n",
    "num_filters_conv1 = 16\n",
    "kernel_size_conv1 = 1 # [height, width]\n",
    "stride_conv1 = 1 # [stride_height, stride_width]\n",
    "kernel_size_pool1 = 1\n",
    "stride_pool1 = 1\n",
    "num_l1 = 100\n",
    "padding_conv1 = 0\n",
    "   \n",
    "def compute_conv_dim(dim_size):\n",
    "    return int((dim_size - kernel_size_conv1 + 2 * padding_conv1) / stride_conv1 + 1)\n",
    "\n",
    "def compute_maxPool_dim(dim_size):\n",
    "    return int((dim_size - kernel_size_pool1 + 2 * padding_conv1) / stride_pool1 + 1)\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #out_dim = (input_dim - filter_dim + 2 * padding) / stride + 1\n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=num_filters_conv1,\n",
    "                             kernel_size=kernel_size_conv1,\n",
    "                             stride=stride_conv1)\n",
    "        \n",
    "       # self.maxPool_1 = MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.conv_out_height = compute_conv_dim(height)\n",
    "        self.conv_out_width = compute_conv_dim(width)\n",
    "      #  self.conv_out_height = compute_maxPool_dim(self.conv_out_height)\n",
    "      #  self.conv_out_width = compute_maxPool_dim(self.conv_out_width)\n",
    "        \n",
    "        # add dropout to network\n",
    "        #self.dropout = Dropout2d(p=0.5)\n",
    "        self.l1_in_features = num_filters_conv1 * self.conv_out_height * self.conv_out_width\n",
    "        #self.l1_in_features = channels * height * width\n",
    "        \n",
    "        self.l_1 = Linear(in_features=self.l1_in_features, \n",
    "                          out_features=num_l1,\n",
    "                          bias=True)\n",
    "        self.l_out = Linear(in_features=num_l1, \n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "    \n",
    "    def forward(self, x): # x.size() = [batch, channel, height, width]\n",
    "        x = relu(self.conv_1(x))\n",
    "        #x = self.maxPool_1(x)\n",
    "        # torch.Tensor.view: http://pytorch.org/docs/master/tensors.html?highlight=view#torch.Tensor.view\n",
    "        #   Returns a new tensor with the same data as the self tensor,\n",
    "        #   but of a different size.\n",
    "        # the size -1 is inferred from other dimensions \n",
    "        x = x.view(-1, self.l1_in_features)\n",
    "        #x = self.dropout(relu(self.l_1(x)))\n",
    "        x = relu(self.l_1(x))\n",
    "        return softmax(self.l_out(x), dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949\n",
      "1\n",
      "641\n"
     ]
    }
   ],
   "source": [
    "print(len(Ms))\n",
    "print(len(Ms[0]))\n",
    "print(len(Ms[0][0].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n",
      "(641, 1, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "index = 0\n",
    "print(len(Ms[0][0].T))\n",
    "    \n",
    "for index in Ms[0][0].T:\n",
    "    x = numpy.array([[index]])\n",
    "    batch.append(x)\n",
    "         \n",
    "batch = numpy.stack(batch, axis=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([641, 27]),\n",
       " tensor([[3.2909e-14, 4.1224e-01, 5.8676e-01,  ..., 2.0630e-10, 3.2550e-20,\n",
       "          1.4548e-05],\n",
       "         [5.9227e-13, 1.5242e-01, 8.4429e-01,  ..., 5.0037e-10, 1.7373e-19,\n",
       "          1.8185e-05],\n",
       "         [1.6091e-12, 1.2815e-01, 8.6611e-01,  ..., 1.4715e-09, 4.8628e-19,\n",
       "          1.0040e-04],\n",
       "         ...,\n",
       "         [3.2388e-17, 5.1573e-02, 9.4828e-01,  ..., 1.0992e-12, 1.1210e-24,\n",
       "          3.3071e-06],\n",
       "         [3.2388e-17, 5.1574e-02, 9.4828e-01,  ..., 1.0992e-12, 1.1210e-24,\n",
       "          3.3071e-06],\n",
       "         [3.2388e-17, 5.1574e-02, 9.4828e-01,  ..., 1.0992e-12, 1.1210e-24,\n",
       "          3.3071e-06]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = net(Variable(torch.from_numpy(batch).float()))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
